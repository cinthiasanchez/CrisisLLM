{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from utils import load_yaml, load_args_promt\n",
    "import torch, constants, json\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = load_yaml('config.multitask.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "\n",
    "settings['train']['batch_size'] = 8\n",
    "settings['train']['max_steps'] = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a {model}. {task}: {categories_description} The tweet text is\n",
      "delimited with triple backticks.  Generate a JSON with the following\n",
      "keys: 'prediction' containing a single predicted category, either\n",
      "{category_list}, and 'explanation' containing the reason (limited to\n",
      "100 characters) for the categorization decision. Tweet text:\n",
      "'''{tweet}'''\n"
     ]
    }
   ],
   "source": [
    "def set_textual_information(promt, promt_settings, tweet):\n",
    "    promt_settings['tweet'] = tweet\n",
    "    return promt.format(**promt_settings)\n",
    "\n",
    "def fill_values_from_row(row, promt_template, promt_settings):\n",
    "    return set_textual_information(\n",
    "        promt_template, promt_settings[row['task']], row['tweet']\n",
    "    )\n",
    "\n",
    "promt_settings = {}\n",
    "\n",
    "if isinstance(settings[\"k-shot\"], dict):\n",
    "    for task, _args_task in settings['k-shot'].items():\n",
    "        promt_settings[task] = load_args_promt(settings, _args_task)\n",
    "else:\n",
    "    with open(f'promts/{settings[\"k-shot\"]}') as buffer:\n",
    "        promt_settings[\n",
    "            settings['tune_name']\n",
    "        ] = json.load(buffer)[settings['k-selected']]\n",
    "    \n",
    "question = constants.QUESTION\n",
    "promt_template = getattr(constants, settings['promt_template'])\n",
    "promt_template = promt_template + \" \" + question\n",
    "\n",
    "print(textwrap.fill(promt_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a social media message classifier. Your task is to categorize\n",
      "a tweet posted during crisis events as one of the following\n",
      "humanitarian categories: - Affected individuals: information about\n",
      "deaths, injuries, missing, trapped, found or displaced people,\n",
      "including personal updates about oneself, family, or others. -\n",
      "Infrastructure and utilities: information about buildings, roads,\n",
      "utilities/services that are damaged, interrupted, restored or\n",
      "operational. - Donations and volunteering: information about needs,\n",
      "requests, queries or offers of money, blood, shelter, supplies (e.g.,\n",
      "food, water, clothing, medical supplies) and/or services by volunteers\n",
      "or professionals. - Caution and advice: information about warnings\n",
      "issued or lifted, guidance and tips. - Sympathy and support: thoughts,\n",
      "prayers, gratitude, sadness, etc. - Other useful information:\n",
      "information NOT covered by any of the above categories. It considers\n",
      "information about fire line/emergency location, flood level,\n",
      "earthquake magnitude, weather, wind, visibility; smoke, ash;\n",
      "adjunctive and meta discussions; other informative messages;\n",
      "information verification, explanation of particular problems The tweet\n",
      "text is delimited with triple backticks.  Generate a JSON with the\n",
      "following keys: 'prediction' containing a single predicted category,\n",
      "either \"Affected individuals\", \"Infrastructure and utilities\",\n",
      "\"Donations and volunteering\", \"Caution and advice\", \"Sympathy and\n",
      "support\" or \"Other useful information\", and 'explanation' containing\n",
      "the reason (limited to 100 characters) for the categorization\n",
      "decision. Tweet text: '''Governor General Quentin Bryce has visited\n",
      "the Blue Mtns today, thanking firefighters #NSWRFS #nswfires\n",
      "http://t.co/gjt1bJPhHR'''\n"
     ]
    }
   ],
   "source": [
    "row = {\n",
    "    'tweet': 'Governor General Quentin Bryce has visited the Blue Mtns today, thanking firefighters #NSWRFS #nswfires http://t.co/gjt1bJPhHR',\n",
    "    'task': 'humanitarian', \n",
    "    'explanation': '',\n",
    "    # 'label': 'Sympathy and support',\n",
    "}\n",
    "\n",
    "_input_sample = fill_values_from_row(row, promt_template, promt_settings)\n",
    "print(textwrap.fill(_input_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    settings['model_id'], token=settings['hub_id'],\n",
    "    model_max_length=settings['model_max_length'],\n",
    "    padding_side=settings['padding_side'],\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "_steps = settings['train'].get('max_steps', -1)\n",
    "\n",
    "_name = settings['model_name'] +'__'+ settings['tune_name']\n",
    "_filename = f\"trained_models/{_name}-v{version}_{_steps}\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    _filename, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"prediction\": \"Affected individuals\", \"explanation\": \"Tweet mentions\n",
      "victims needing help and food.\"}\n"
     ]
    }
   ],
   "source": [
    "_inputs = tokenizer(\n",
    "    [_input_sample], return_tensors=\"pt\",\n",
    "    truncation=True, padding=True\n",
    ")\n",
    "\n",
    "_results = model.generate(\n",
    "    input_ids=_inputs[\"input_ids\"].to('cuda'), \n",
    "    attention_mask=_inputs[\"attention_mask\"].to('cuda'), \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, temperature=settings['temperature']\n",
    ")\n",
    "\n",
    "reponse = tokenizer.decode(\n",
    "    _results[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(textwrap.fill(reponse.split('[/]')[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
